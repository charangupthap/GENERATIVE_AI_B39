{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHLpuwDp87gJAUSwPHsJ7z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/charangupthap/GENERATIVE_AI_B39/blob/main/Untitled11_1ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "a2OdT3lO_Tpt",
        "outputId": "ed21f66c-00b3-4bc0-e8b2-e1a95fa6f122"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://drive.google.com/file/d/15v2CmnA-BqLMU0E86whGEnSl6Wbv4mKW/export?export=download\n",
            "Error loading or preprocessing data: URL fetch failure on https://drive.google.com/file/d/15v2CmnA-BqLMU0E86whGEnSl6Wbv4mKW/export?export=download: 404 -- Not Found\n",
            "Failed to load data. Please check the URLs and your internet connection.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training Model 1 ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'shape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2d9acca83d22>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Training Model 1 ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m \u001b[0mhistory_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;31m# --- 2. Calculate Accuracy ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/data_adapter_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_data_cardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         msg = (\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from PIL import Image\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "# Define the image dimensions\n",
        "img_height = 64\n",
        "img_width = 64\n",
        "num_classes = 4  # Assuming 4 classes based on potential image categories (update if different)\n",
        "\n",
        "# --- Load and Preprocess Data ---\n",
        "def load_and_preprocess_data(train_url, val_url):\n",
        "    \"\"\"Loads, preprocesses, and splits the training and validation data.\"\"\"\n",
        "    try:\n",
        "        # Download and extract training data\n",
        "        train_file = tf.keras.utils.get_file(\n",
        "            'training_data.tar.gz', train_url, extract=True\n",
        "        )\n",
        "        train_dir = os.path.join(os.path.dirname(train_file), 'training_data')\n",
        "        train_images = []\n",
        "        train_labels = []\n",
        "        for label in os.listdir(train_dir):\n",
        "            label_path = os.path.join(train_dir, label)\n",
        "            if os.path.isdir(label_path):\n",
        "                for img_file in os.listdir(label_path):\n",
        "                    img_path = os.path.join(label_path, img_file)\n",
        "                    try:\n",
        "                        img = Image.open(img_path).resize((img_width, img_height)).convert('RGB')\n",
        "                        img_array = np.array(img) / 255.0\n",
        "                        train_images.append(img_array)\n",
        "                        train_labels.append(label)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading image: {img_path} - {e}\")\n",
        "        train_images = np.array(train_images)\n",
        "        train_labels = np.array(train_labels)\n",
        "\n",
        "        # Download and extract validation data\n",
        "        val_file = tf.keras.utils.get_file(\n",
        "            'validation_data.tar.gz', val_url, extract=True\n",
        "        )\n",
        "        val_dir = os.path.join(os.path.dirname(val_file), 'validation_data')\n",
        "        val_images = []\n",
        "        val_labels = []\n",
        "        for label in os.listdir(val_dir):\n",
        "            label_path = os.path.join(val_dir, label)\n",
        "            if os.path.isdir(label_path):\n",
        "                for img_file in os.listdir(label_path):\n",
        "                    img_path = os.path.join(label_path, img_file)\n",
        "                    try:\n",
        "                        img = Image.open(img_path).resize((img_width, img_height)).convert('RGB')\n",
        "                        img_array = np.array(img) / 255.0\n",
        "                        val_images.append(img_array)\n",
        "                        val_labels.append(label)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading image: {img_path} - {e}\")\n",
        "        val_images = np.array(val_images)\n",
        "        val_labels = np.array(val_labels)\n",
        "\n",
        "        # Encode labels\n",
        "        label_encoder = LabelEncoder()\n",
        "        train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
        "        val_labels_encoded = label_encoder.transform(val_labels)\n",
        "        train_labels_categorical = tf.keras.utils.to_categorical(train_labels_encoded, num_classes=num_classes)\n",
        "        val_labels_categorical = tf.keras.utils.to_categorical(val_labels_encoded, num_classes=num_classes)\n",
        "\n",
        "        return train_images, train_labels_categorical, val_images, val_labels_categorical\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or preprocessing data: {e}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "train_data_url = \"https://drive.google.com/file/d/15v2CmnA-BqLMU0E86whGEnSl6Wbv4mKW/view?usp=drive_link\"\n",
        "validation_data_url = \"https://drive.google.com/file/d/1uSswsVLN1ASLWd4KSwocYvGaE0rgXv9l/view?usp=sharing\"\n",
        "\n",
        "train_images, train_labels, val_images, val_labels = load_and_preprocess_data(\n",
        "    train_data_url.replace('/view?usp=drive_link', '/export?export=download'),\n",
        "    validation_data_url.replace('/view?usp=drive_link', '/export?export=download')\n",
        ")\n",
        "\n",
        "if train_images is None:\n",
        "    print(\"Failed to load data. Please check the URLs and your internet connection.\")\n",
        "    exit()\n",
        "\n",
        "# --- 1. Build and Train the Initial CNN ---\n",
        "def build_cnn_model_1(input_shape=(img_height, img_width, 3), num_classes=num_classes):\n",
        "    model = keras.Sequential([\n",
        "        layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(filters=256, kernel_size=(3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(units=256, activation='relu'),\n",
        "        layers.Dense(units=num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model_1 = build_cnn_model_1()\n",
        "\n",
        "# Compile the model\n",
        "optimizer = tf.keras.optimizers.Adadelta()\n",
        "model_1.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Training parameters\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "# Train the model\n",
        "print(\"\\n--- Training Model 1 ---\")\n",
        "history_1 = model_1.fit(train_images, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(val_images, val_labels))\n",
        "\n",
        "# --- 2. Calculate Accuracy ---\n",
        "def calculate_accuracy(model, images, labels, set_name=\"Data\"):\n",
        "    \"\"\"Calculates and prints the accuracy of the model on the given dataset.\"\"\"\n",
        "    _, accuracy = model.evaluate(images, labels, verbose=0)\n",
        "    print(f\"Accuracy on {set_name}: {accuracy:.4f}\")\n",
        "    return accuracy\n",
        "\n",
        "print(\"\\n--- Evaluating Model 1 ---\")\n",
        "train_accuracy_1 = calculate_accuracy(model_1, train_images, train_labels, \"Training Data\")\n",
        "test_accuracy_1 = calculate_accuracy(model_1, val_images, val_labels, \"Testing Data\")\n",
        "\n",
        "# --- 3. Change Architecture to Identify Best Architecture ---\n",
        "\n",
        "def build_cnn_model_2(input_shape=(img_height, img_width, 3), num_classes=num_classes):\n",
        "    \"\"\"Modified CNN architecture with more convolutional layers and dropout.\"\"\"\n",
        "    model = keras.Sequential([\n",
        "        layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
        "        layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(units=512, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(units=num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_cnn_model_3(input_shape=(img_height, img_width, 3), num_classes=num_classes):\n",
        "    \"\"\"Another modified CNN architecture with fewer layers but larger filters initially.\"\"\"\n",
        "    model = keras.Sequential([\n",
        "        layers.Conv2D(filters=96, kernel_size=(5, 5), activation='relu', padding='same', input_shape=input_shape),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(filters=512, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(units=256, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(units=num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Train and evaluate Model 2\n",
        "model_2 = build_cnn_model_2()\n",
        "model_2.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "print(\"\\n--- Training Model 2 ---\")\n",
        "history_2 = model_2.fit(train_images, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(val_images, val_labels))\n",
        "print(\"\\n--- Evaluating Model 2 ---\")\n",
        "train_accuracy_2 = calculate_accuracy(model_2, train_images, train_labels, \"Training Data\")\n",
        "test_accuracy_2 = calculate_accuracy(model_2, val_images, val_labels, \"Testing Data\")\n",
        "\n",
        "# Train and evaluate Model 3\n",
        "model_3 = build_cnn_model_3()\n",
        "model_3.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "print(\"\\n--- Training Model 3 ---\")\n",
        "history_3 = model_3.fit(train_images, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(val_images, val_labels))\n",
        "print(\"\\n--- Evaluating Model 3 ---\")\n",
        "train_accuracy_3 = calculate_accuracy(model_3, train_images, train_labels, \"Training Data\")\n",
        "test_accuracy_3 = calculate_accuracy(model_3, val_images, val_labels, \"Testing Data\")\n",
        "\n",
        "# Identify the best architecture based on testing accuracy\n",
        "best_accuracy = max(test_accuracy_1, test_accuracy_2, test_accuracy_3)\n",
        "if best_accuracy == test_accuracy_1:\n",
        "    best_architecture = \"Model 1 (as per Table 1)\"\n",
        "elif best_accuracy == test_accuracy_2:\n",
        "    best_architecture = \"Model 2 (Modified)\"\n",
        "else:\n",
        "    best_architecture = \"Model 3 (Modified)\"\n",
        "\n",
        "print(f\"\\n--- Best Architecture ---\")\n",
        "print(f\"The best architecture in terms of testing accuracy is: {best_architecture} with a testing accuracy of {best_accuracy:.4f}\")"
      ]
    }
  ]
}